{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Downloading PyPDF2-2.11.1-py3-none-any.whl (220 kB)\n",
      "     ------------------------------------ 220.4/220.4 kB 560.3 kB/s eta 0:00:00\n",
      "Collecting typing-extensions>=3.10.0.0\n",
      "  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "Installing collected packages: typing-extensions, PyPDF2\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.7.4.3\n",
      "    Uninstalling typing-extensions-3.7.4.3:\n",
      "      Successfully uninstalled typing-extensions-3.7.4.3\n",
      "Successfully installed PyPDF2-2.11.1 typing-extensions-4.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing nltk library\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing PyPDF2\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pdf = open(r\"Motivation_and_basic_concepts.pdf\",'rb')\n",
    "pdf_read = PyPDF2.PdfFileReader(Pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "print(pdf_read.numPages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/Type': '/Page',\n",
       " '/Parent': {'/Type': '/Pages',\n",
       "  '/Kids': [IndirectObject(2, 0, 1606858237120),\n",
       "   IndirectObject(13, 0, 1606858237120),\n",
       "   IndirectObject(21, 0, 1606858237120),\n",
       "   IndirectObject(30, 0, 1606858237120),\n",
       "   IndirectObject(37, 0, 1606858237120),\n",
       "   IndirectObject(43, 0, 1606858237120),\n",
       "   IndirectObject(50, 0, 1606858237120),\n",
       "   IndirectObject(58, 0, 1606858237120),\n",
       "   IndirectObject(70, 0, 1606858237120),\n",
       "   IndirectObject(74, 0, 1606858237120),\n",
       "   IndirectObject(80, 0, 1606858237120),\n",
       "   IndirectObject(89, 0, 1606858237120),\n",
       "   IndirectObject(93, 0, 1606858237120),\n",
       "   IndirectObject(98, 0, 1606858237120)],\n",
       "  '/Count': 14},\n",
       " '/MediaBox': [0, 0, 595.275591, 841.889764],\n",
       " '/Contents': {'/Filter': '/FlateDecode'},\n",
       " '/Group': {'/Type': '/Group',\n",
       "  '/S': '/Transparency',\n",
       "  '/I': True,\n",
       "  '/CS': '/DeviceRGB'},\n",
       " '/Resources': {'/ExtGState': {'/a0': {'/CA': 1, '/ca': 1}},\n",
       "  '/Font': {'/f-1-0': {'/Type': '/Font',\n",
       "    '/Subtype': '/Type1',\n",
       "    '/BaseFont': '/ESFNUE+NimbusRomNo9L-Regu',\n",
       "    '/FirstChar': 32,\n",
       "    '/LastChar': 150,\n",
       "    '/FontDescriptor': {'/Type': '/FontDescriptor',\n",
       "     '/FontName': '/ESFNUE+NimbusRomNo9L-Regu',\n",
       "     '/Flags': 4,\n",
       "     '/FontBBox': [-168, -281, 1000, 924],\n",
       "     '/ItalicAngle': 0,\n",
       "     '/Ascent': 924,\n",
       "     '/Descent': -281,\n",
       "     '/CapHeight': 924,\n",
       "     '/StemV': 80,\n",
       "     '/StemH': 80,\n",
       "     '/FontFile': {'/Filter': '/FlateDecode',\n",
       "      '/Length1': 2895,\n",
       "      '/Length2': 15443,\n",
       "      '/Length3': 533}},\n",
       "    '/Encoding': '/WinAnsiEncoding',\n",
       "    '/Widths': [0,\n",
       "     333,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     833,\n",
       "     778,\n",
       "     0,\n",
       "     333,\n",
       "     333,\n",
       "     0,\n",
       "     0,\n",
       "     250,\n",
       "     333,\n",
       "     250,\n",
       "     278,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     278,\n",
       "     278,\n",
       "     0,\n",
       "     564,\n",
       "     0,\n",
       "     444,\n",
       "     0,\n",
       "     722,\n",
       "     667,\n",
       "     667,\n",
       "     722,\n",
       "     611,\n",
       "     556,\n",
       "     722,\n",
       "     722,\n",
       "     333,\n",
       "     0,\n",
       "     722,\n",
       "     611,\n",
       "     889,\n",
       "     722,\n",
       "     722,\n",
       "     556,\n",
       "     722,\n",
       "     667,\n",
       "     556,\n",
       "     611,\n",
       "     722,\n",
       "     722,\n",
       "     944,\n",
       "     0,\n",
       "     722,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     444,\n",
       "     500,\n",
       "     444,\n",
       "     500,\n",
       "     444,\n",
       "     333,\n",
       "     500,\n",
       "     500,\n",
       "     278,\n",
       "     278,\n",
       "     500,\n",
       "     278,\n",
       "     778,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     333,\n",
       "     389,\n",
       "     278,\n",
       "     500,\n",
       "     500,\n",
       "     722,\n",
       "     500,\n",
       "     500,\n",
       "     444,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     333,\n",
       "     444,\n",
       "     444,\n",
       "     0,\n",
       "     500],\n",
       "    '/ToUnicode': {'/Filter': '/FlateDecode'}},\n",
       "   '/f-1-1': {'/Type': '/Font',\n",
       "    '/Subtype': '/Type1',\n",
       "    '/BaseFont': '/RPBPCT+NimbusRomNo9L-Regu',\n",
       "    '/FirstChar': 0,\n",
       "    '/LastChar': 2,\n",
       "    '/FontDescriptor': {'/Type': '/FontDescriptor',\n",
       "     '/FontName': '/RPBPCT+NimbusRomNo9L-Regu',\n",
       "     '/Flags': 4,\n",
       "     '/FontBBox': [-168, -281, 1000, 924],\n",
       "     '/ItalicAngle': 0,\n",
       "     '/Ascent': 924,\n",
       "     '/Descent': -281,\n",
       "     '/CapHeight': 924,\n",
       "     '/StemV': 80,\n",
       "     '/StemH': 80,\n",
       "     '/FontFile': {'/Filter': '/FlateDecode',\n",
       "      '/Length1': 1688,\n",
       "      '/Length2': 1877,\n",
       "      '/Length3': 533}},\n",
       "    '/Widths': [250, 556, 556],\n",
       "    '/ToUnicode': {'/Filter': '/FlateDecode'}}}}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page = pdf_read.getPage(8)\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tex = page.extractText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopword = set(stopwords.words('english'))\n",
    "stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2.',\n",
       " 'Summarise time series data.',\n",
       " 'For example, in the analysis of Electroencephalography (EEG)\\nrecordings, how can we decide whether the subject is “healthy” or not?',\n",
       " 'Or how can we\\ndecide if the economy in US (see the examples above) has been evolving “signiﬁcantly\\ndifferently” from that, say, in China?',\n",
       " 'Or, more generally, given a time series, how can we\\ndescribe and summarise its evolution?',\n",
       " '3.',\n",
       " 'Control the evolution of a time series.',\n",
       " 'This is not quite the same as forecasting, where we\\ndo not intervene in the process in any way.',\n",
       " 'As an example of the control problem, consider\\nthe global temperature data: is “global warming” really happening, and if so, what impacts\\nthe temperature and how can we eliminate or suppress those factors?',\n",
       " 'As expected, there are often a variety of ways in which those questions can be answered, and many\\nof them do not formally involve statistics at all: for example, people often debate expected trends\\nin house prices and investment opportunities, express their informal views about global warming,\\nor use techniques originating from computer science (e.g.',\n",
       " 'pattern recognition) to aid medical di-\\nagnosis in neuroscience.',\n",
       " 'So do we need statistics in time series analysis?',\n",
       " 'The answer is not necessarily, but there are good arguments why the statistical approach may\\noften be very useful.',\n",
       " '1.',\n",
       " 'Firstly, even those informal approaches to time series analysis are in fact often statistical in\\nnature, sometimes in a “hidden” way: for example, people’s subjective views about time se-\\nries can often be formally formulated as priors in Bayesian statistics, and informal forecasts\\nwhich we often encounter in the media are in fact often instances of simple statistical fore-\\ncasting procedures, such as trend extrapolation.',\n",
       " 'Also, frequently, techniques originating in\\ncomputer science (such as: neural networks, machine learning, pattern recognition, artiﬁcial\\nintelligence) often have their counterparts in statistics, which do exactly the same thing but\\nare named differently.',\n",
       " '2.',\n",
       " 'The above-mentioned tasks: forecasting, understanding the structure of time series, and time\\nseries control, have inherent uncertainty about them, which makes probability and statistics\\na natural tool for describing them.',\n",
       " '(a) Accurate forecasting is impossible.',\n",
       " 'For example, rather than saying “tomorrow’s value\\nwill be exactly 2.745” it often makes more sense to say “tomorrow’s value will be\\naround 2.745”, but then there is a chance that we will still be wrong, so our forecasts,\\neven those informal ones, will often be of the form “tomorrow’s value will probably\\nbe around 2.745”, which is already in the territory of probability, since it contains a\\nnatural statement of uncertainty.',\n",
       " '(b) It is often impossible to build exact deterministic models describing their structure.',\n",
       " 'Indeed, if we had a correct deterministic model, we would be able to predict the evolu-\\ntion of the time series exactly, but since we are not able to do that, it means that we do\\nnot have the exact model.',\n",
       " 'Often, probabilistic models make more sense: for example,\\n“tomorrow’s value is about a half of today’s value plus a term which is best described\\nas random, i.e.',\n",
       " 'there is no clear pattern in its values from one day to another”.',\n",
       " '(c) In the issue of time series control, one natural task that often needs to be performed is to\\nunderstand what factors affect the evolution of the series.',\n",
       " 'But this is often impossible\\nto specify exactly: it is unlikely that any one factor (out of the ones we are considering),\\nor indeed their combination, is fully responsible for the evolution of the time series.',\n",
       " '14']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = sent_tokenize(tex)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in tokens:\n",
    "    wordlist = word_tokenize(j)\n",
    "    wordlist = [w for w in wordlist if w not in stopword]\n",
    "    tag = nltk.pos_tag(wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('14', 'CD')]\n"
     ]
    }
   ],
   "source": [
    "print(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S 14/CD)\n"
     ]
    }
   ],
   "source": [
    "chunk_entities = nltk.ne_chunk(tag)\n",
    "print(chunk_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "namedentities = []\n",
    "for tagged_tree in chunk_entities:\n",
    "    if hasattr(tagged_tree, 'label'):\n",
    "        en_name=''.join(c[0] for c in  tagged_tree.leaves())\n",
    "        en_type= tagged_tree.label()\n",
    "        namedentities.append((en_name,en_type))\n",
    "print(namedentities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      ".\n",
      "summaris\n",
      "time\n",
      "seri\n",
      "data\n",
      ".\n",
      "for\n",
      "exampl\n",
      ",\n",
      "in\n",
      "the\n",
      "analysi\n",
      "of\n",
      "electroencephalographi\n",
      "(\n",
      "eeg\n",
      ")\n",
      "record\n",
      ",\n",
      "how\n",
      "can\n",
      "we\n",
      "decid\n",
      "whether\n",
      "the\n",
      "subject\n",
      "is\n",
      "“\n",
      "healthi\n",
      "”\n",
      "or\n",
      "not\n",
      "?\n",
      "or\n",
      "how\n",
      "can\n",
      "we\n",
      "decid\n",
      "if\n",
      "the\n",
      "economi\n",
      "in\n",
      "us\n",
      "(\n",
      "see\n",
      "the\n",
      "exampl\n",
      "abov\n",
      ")\n",
      "ha\n",
      "been\n",
      "evolv\n",
      "“\n",
      "signiﬁcantli\n",
      "differ\n",
      "”\n",
      "from\n",
      "that\n",
      ",\n",
      "say\n",
      ",\n",
      "in\n",
      "china\n",
      "?\n",
      "or\n",
      ",\n",
      "more\n",
      "gener\n",
      ",\n",
      "given\n",
      "a\n",
      "time\n",
      "seri\n",
      ",\n",
      "how\n",
      "can\n",
      "we\n",
      "describ\n",
      "and\n",
      "summaris\n",
      "it\n",
      "evolut\n",
      "?\n",
      "3\n",
      ".\n",
      "control\n",
      "the\n",
      "evolut\n",
      "of\n",
      "a\n",
      "time\n",
      "seri\n",
      ".\n",
      "thi\n",
      "is\n",
      "not\n",
      "quit\n",
      "the\n",
      "same\n",
      "as\n",
      "forecast\n",
      ",\n",
      "where\n",
      "we\n",
      "do\n",
      "not\n",
      "interven\n",
      "in\n",
      "the\n",
      "process\n",
      "in\n",
      "ani\n",
      "way\n",
      ".\n",
      "as\n",
      "an\n",
      "exampl\n",
      "of\n",
      "the\n",
      "control\n",
      "problem\n",
      ",\n",
      "consid\n",
      "the\n",
      "global\n",
      "temperatur\n",
      "data\n",
      ":\n",
      "is\n",
      "“\n",
      "global\n",
      "warm\n",
      "”\n",
      "realli\n",
      "happen\n",
      ",\n",
      "and\n",
      "if\n",
      "so\n",
      ",\n",
      "what\n",
      "impact\n",
      "the\n",
      "temperatur\n",
      "and\n",
      "how\n",
      "can\n",
      "we\n",
      "elimin\n",
      "or\n",
      "suppress\n",
      "those\n",
      "factor\n",
      "?\n",
      "as\n",
      "expect\n",
      ",\n",
      "there\n",
      "are\n",
      "often\n",
      "a\n",
      "varieti\n",
      "of\n",
      "way\n",
      "in\n",
      "which\n",
      "those\n",
      "question\n",
      "can\n",
      "be\n",
      "answer\n",
      ",\n",
      "and\n",
      "mani\n",
      "of\n",
      "them\n",
      "do\n",
      "not\n",
      "formal\n",
      "involv\n",
      "statist\n",
      "at\n",
      "all\n",
      ":\n",
      "for\n",
      "exampl\n",
      ",\n",
      "peopl\n",
      "often\n",
      "debat\n",
      "expect\n",
      "trend\n",
      "in\n",
      "hous\n",
      "price\n",
      "and\n",
      "invest\n",
      "opportun\n",
      ",\n",
      "express\n",
      "their\n",
      "inform\n",
      "view\n",
      "about\n",
      "global\n",
      "warm\n",
      ",\n",
      "or\n",
      "use\n",
      "techniqu\n",
      "origin\n",
      "from\n",
      "comput\n",
      "scienc\n",
      "(\n",
      "e.g\n",
      ".\n",
      "pattern\n",
      "recognit\n",
      ")\n",
      "to\n",
      "aid\n",
      "medic\n",
      "di-\n",
      "agnosi\n",
      "in\n",
      "neurosci\n",
      ".\n",
      "so\n",
      "do\n",
      "we\n",
      "need\n",
      "statist\n",
      "in\n",
      "time\n",
      "seri\n",
      "analysi\n",
      "?\n",
      "the\n",
      "answer\n",
      "is\n",
      "not\n",
      "necessarili\n",
      ",\n",
      "but\n",
      "there\n",
      "are\n",
      "good\n",
      "argument\n",
      "whi\n",
      "the\n",
      "statist\n",
      "approach\n",
      "may\n",
      "often\n",
      "be\n",
      "veri\n",
      "use\n",
      ".\n",
      "1\n",
      ".\n",
      "firstli\n",
      ",\n",
      "even\n",
      "those\n",
      "inform\n",
      "approach\n",
      "to\n",
      "time\n",
      "seri\n",
      "analysi\n",
      "are\n",
      "in\n",
      "fact\n",
      "often\n",
      "statist\n",
      "in\n",
      "natur\n",
      ",\n",
      "sometim\n",
      "in\n",
      "a\n",
      "“\n",
      "hidden\n",
      "”\n",
      "way\n",
      ":\n",
      "for\n",
      "exampl\n",
      ",\n",
      "peopl\n",
      "’\n",
      "s\n",
      "subject\n",
      "view\n",
      "about\n",
      "time\n",
      "se-\n",
      "rie\n",
      "can\n",
      "often\n",
      "be\n",
      "formal\n",
      "formul\n",
      "as\n",
      "prior\n",
      "in\n",
      "bayesian\n",
      "statist\n",
      ",\n",
      "and\n",
      "inform\n",
      "forecast\n",
      "which\n",
      "we\n",
      "often\n",
      "encount\n",
      "in\n",
      "the\n",
      "media\n",
      "are\n",
      "in\n",
      "fact\n",
      "often\n",
      "instanc\n",
      "of\n",
      "simpl\n",
      "statist\n",
      "fore-\n",
      "cast\n",
      "procedur\n",
      ",\n",
      "such\n",
      "as\n",
      "trend\n",
      "extrapol\n",
      ".\n",
      "also\n",
      ",\n",
      "frequent\n",
      ",\n",
      "techniqu\n",
      "origin\n",
      "in\n",
      "comput\n",
      "scienc\n",
      "(\n",
      "such\n",
      "as\n",
      ":\n",
      "neural\n",
      "network\n",
      ",\n",
      "machin\n",
      "learn\n",
      ",\n",
      "pattern\n",
      "recognit\n",
      ",\n",
      "artiﬁci\n",
      "intellig\n",
      ")\n",
      "often\n",
      "have\n",
      "their\n",
      "counterpart\n",
      "in\n",
      "statist\n",
      ",\n",
      "which\n",
      "do\n",
      "exactli\n",
      "the\n",
      "same\n",
      "thing\n",
      "but\n",
      "are\n",
      "name\n",
      "differ\n",
      ".\n",
      "2\n",
      ".\n",
      "the\n",
      "above-ment\n",
      "task\n",
      ":\n",
      "forecast\n",
      ",\n",
      "understand\n",
      "the\n",
      "structur\n",
      "of\n",
      "time\n",
      "seri\n",
      ",\n",
      "and\n",
      "time\n",
      "seri\n",
      "control\n",
      ",\n",
      "have\n",
      "inher\n",
      "uncertainti\n",
      "about\n",
      "them\n",
      ",\n",
      "which\n",
      "make\n",
      "probabl\n",
      "and\n",
      "statist\n",
      "a\n",
      "natur\n",
      "tool\n",
      "for\n",
      "describ\n",
      "them\n",
      ".\n",
      "(\n",
      "a\n",
      ")\n",
      "accur\n",
      "forecast\n",
      "is\n",
      "imposs\n",
      ".\n",
      "for\n",
      "exampl\n",
      ",\n",
      "rather\n",
      "than\n",
      "say\n",
      "“\n",
      "tomorrow\n",
      "’\n",
      "s\n",
      "valu\n",
      "will\n",
      "be\n",
      "exactli\n",
      "2.745\n",
      "”\n",
      "it\n",
      "often\n",
      "make\n",
      "more\n",
      "sens\n",
      "to\n",
      "say\n",
      "“\n",
      "tomorrow\n",
      "’\n",
      "s\n",
      "valu\n",
      "will\n",
      "be\n",
      "around\n",
      "2.745\n",
      "”\n",
      ",\n",
      "but\n",
      "then\n",
      "there\n",
      "is\n",
      "a\n",
      "chanc\n",
      "that\n",
      "we\n",
      "will\n",
      "still\n",
      "be\n",
      "wrong\n",
      ",\n",
      "so\n",
      "our\n",
      "forecast\n",
      ",\n",
      "even\n",
      "those\n",
      "inform\n",
      "one\n",
      ",\n",
      "will\n",
      "often\n",
      "be\n",
      "of\n",
      "the\n",
      "form\n",
      "“\n",
      "tomorrow\n",
      "’\n",
      "s\n",
      "valu\n",
      "will\n",
      "probabl\n",
      "be\n",
      "around\n",
      "2.745\n",
      "”\n",
      ",\n",
      "which\n",
      "is\n",
      "alreadi\n",
      "in\n",
      "the\n",
      "territori\n",
      "of\n",
      "probabl\n",
      ",\n",
      "sinc\n",
      "it\n",
      "contain\n",
      "a\n",
      "natur\n",
      "statement\n",
      "of\n",
      "uncertainti\n",
      ".\n",
      "(\n",
      "b\n",
      ")\n",
      "it\n",
      "is\n",
      "often\n",
      "imposs\n",
      "to\n",
      "build\n",
      "exact\n",
      "determinist\n",
      "model\n",
      "describ\n",
      "their\n",
      "structur\n",
      ".\n",
      "inde\n",
      ",\n",
      "if\n",
      "we\n",
      "had\n",
      "a\n",
      "correct\n",
      "determinist\n",
      "model\n",
      ",\n",
      "we\n",
      "would\n",
      "be\n",
      "abl\n",
      "to\n",
      "predict\n",
      "the\n",
      "evolu-\n",
      "tion\n",
      "of\n",
      "the\n",
      "time\n",
      "seri\n",
      "exactli\n",
      ",\n",
      "but\n",
      "sinc\n",
      "we\n",
      "are\n",
      "not\n",
      "abl\n",
      "to\n",
      "do\n",
      "that\n",
      ",\n",
      "it\n",
      "mean\n",
      "that\n",
      "we\n",
      "do\n",
      "not\n",
      "have\n",
      "the\n",
      "exact\n",
      "model\n",
      ".\n",
      "often\n",
      ",\n",
      "probabilist\n",
      "model\n",
      "make\n",
      "more\n",
      "sens\n",
      ":\n",
      "for\n",
      "exampl\n",
      ",\n",
      "“\n",
      "tomorrow\n",
      "’\n",
      "s\n",
      "valu\n",
      "is\n",
      "about\n",
      "a\n",
      "half\n",
      "of\n",
      "today\n",
      "’\n",
      "s\n",
      "valu\n",
      "plu\n",
      "a\n",
      "term\n",
      "which\n",
      "is\n",
      "best\n",
      "describ\n",
      "as\n",
      "random\n",
      ",\n",
      "i.e\n",
      ".\n",
      "there\n",
      "is\n",
      "no\n",
      "clear\n",
      "pattern\n",
      "in\n",
      "it\n",
      "valu\n",
      "from\n",
      "one\n",
      "day\n",
      "to\n",
      "anoth\n",
      "”\n",
      ".\n",
      "(\n",
      "c\n",
      ")\n",
      "in\n",
      "the\n",
      "issu\n",
      "of\n",
      "time\n",
      "seri\n",
      "control\n",
      ",\n",
      "one\n",
      "natur\n",
      "task\n",
      "that\n",
      "often\n",
      "need\n",
      "to\n",
      "be\n",
      "perform\n",
      "is\n",
      "to\n",
      "understand\n",
      "what\n",
      "factor\n",
      "affect\n",
      "the\n",
      "evolut\n",
      "of\n",
      "the\n",
      "seri\n",
      ".\n",
      "but\n",
      "thi\n",
      "is\n",
      "often\n",
      "imposs\n",
      "to\n",
      "specifi\n",
      "exactli\n",
      ":\n",
      "it\n",
      "is\n",
      "unlik\n",
      "that\n",
      "ani\n",
      "one\n",
      "factor\n",
      "(\n",
      "out\n",
      "of\n",
      "the\n",
      "one\n",
      "we\n",
      "are\n",
      "consid\n",
      ")\n",
      ",\n",
      "or\n",
      "inde\n",
      "their\n",
      "combin\n",
      ",\n",
      "is\n",
      "fulli\n",
      "respons\n",
      "for\n",
      "the\n",
      "evolut\n",
      "of\n",
      "the\n",
      "time\n",
      "seri\n",
      ".\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "PS=PorterStemmer()\n",
    "word=word_tokenize(tex)\n",
    "for w in word:\n",
    "    print(PS.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
